{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "doc",
      "metadata": {},
      "source": [
        "# Documentation — Hashtags and Folium Map\n",
        "\n",
        "Computes green share (parks + forest) for neighborhoods and districts and produces:\n",
        "- CSVs with `hashtags` like `#low_green_share` / `#average_green_share` / `#high_green_share`.\n",
        "- Folium maps visualizing the categories.\n",
        "\n",
        "## Hashtag logic\n",
        "- Neighborhoods:\n",
        "  - Sum park polygon area (km²) from `parks.csv` per neighborhood → `green_area_km2`.\n",
        "  - Compute `green_share` = `green_area_km2` / neighborhood `area_km2`.\n",
        "  - Label by a median band: `< median-0.03` → `below average`, `> median+0.03` → `above average`, else `average`.\n",
        "  - Map to tags via `parks_tag_nei()` → `#low_green_share` / `#average_green_share` / `#high_green_share`.\n",
        "- Districts:\n",
        "  - Merge parks area (sum of neighborhoods) with regional statistics forest area (ha) and total area (ha).\n",
        "  - `green_share` = (parks_ha + forest_ha) / total_area_ha.\n",
        "  - Apply the same median±band labeling and map to the same three tags.\n",
        "\n",
        "## Folium visualization\n",
        "- Build a `folium.Map` and add a `GeoJson` layer colored by the green-share class.\n",
        "- Tooltips show district/neighborhood and class; saved to `labels_with_visualization/outputs/parks_map_*.html`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8793fd5",
      "metadata": {},
      "source": [
        "# Parks Labels — Neighborhoods and Districts\n",
        "Self-contained notebook to compute green share labels and export CSVs with schemas:\n",
        "- neighborhoods: [district, neighborhood, hashtags, source]\n",
        "- districts: [district, hashtags, source]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a9ec34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "\n",
        "# Config (resolve project root so paths work from this notebook)\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT/'data').exists():\n",
        "    ROOT = ROOT.parent\n",
        "if not (ROOT/'data').exists():\n",
        "    raise FileNotFoundError(f\"Couldn't locate 'data' directory from {Path.cwd()}\")\n",
        "RAW_DIR = ROOT/'data'/'raw'\n",
        "NEI_PATH = ROOT/'data'/'neighborhoods.geojson'\n",
        "OUT_DIR = Path('outputs'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PARKS_CSV = RAW_DIR/'parks.csv'\n",
        "REG_STATS_CSV = RAW_DIR/'regional_statistics.csv'\n",
        "\n",
        "def ensure_wgs84(gdf):\n",
        "    if gdf.crs is None: return gdf.set_crs(4326)\n",
        "    return gdf.to_crs(4326) if gdf.crs.to_epsg()!=4326 else gdf\n",
        "def compute_area_km2(gdf):\n",
        "    gutm = ensure_wgs84(gdf).to_crs(25833)\n",
        "    gdf['area_km2'] = (gutm.geometry.area/1e6).values\n",
        "    gdf['area_eff_km2'] = gdf['area_km2'].clip(lower=0.20)\n",
        "    return gdf\n",
        "def hashtags(theme, label):\n",
        "    norm = str(label).strip().lower().replace(' ','-')\n",
        "    return ';'.join([f'#'+theme, f'#'+norm])\n",
        "# Map parks green-share categories to single tags (neighborhood-level)\n",
        "def parks_tag_nei(lbl: str) -> str:\n",
        "    lab = str(lbl).strip().lower()\n",
        "    if lab == 'below average': return '#low_green_share'\n",
        "    if lab == 'above average': return '#high_green_share'\n",
        "    return '#average_green_share'\n",
        "\n",
        "GDF = compute_area_km2(ensure_wgs84(gpd.read_file(NEI_PATH)))\n",
        "parks_raw = pd.read_csv(PARKS_CSV)\n",
        "parks_raw['green_area_km2'] = parks_raw['size_sqm'].astype(float)/1e6\n",
        "agg = parks_raw.groupby(['district_id','neighborhood'], dropna=False)['green_area_km2'].sum().reset_index()\n",
        "nei = GDF[['district_id','district','neighborhood','area_km2']].merge(agg, on=['district_id','neighborhood'], how='left').fillna({'green_area_km2':0.0})\n",
        "nei['green_share'] = (nei['green_area_km2']/nei['area_km2']).replace([np.inf,-np.inf], np.nan)\n",
        "med = np.nanmedian(nei['green_share']); lower, upper = med-0.03, med+0.03\n",
        "def label_g(v):\n",
        "    if np.isnan(v): return 'average'\n",
        "    if v<lower: return 'below average'\n",
        "    if v>upper: return 'above average'\n",
        "    return 'average'\n",
        "nei['green_share_label'] = nei['green_share'].apply(label_g)\n",
        "nei_out = pd.DataFrame({\n",
        "    'district': nei['district'],\n",
        "    'neighborhood': nei['neighborhood'],\n",
        "    'hashtags': [parks_tag_nei(v) for v in nei['green_share_label']],\n",
        "    'source': 'parks.ipynb:rule-based'\n",
        "})\n",
        "nei_out.to_csv(OUT_DIR/'neighborhood_labels_parks.csv', index=False)\n",
        "# Append to combined neighborhoods long (idempotent)\n",
        "nei_long_path = OUT_DIR/'berlin_neighborhoods_labels_long.csv'\n",
        "if nei_long_path.exists():\n",
        "    _old = pd.read_csv(nei_long_path)\n",
        "    _new = pd.concat([_old, nei_out], ignore_index=True)\n",
        "else:\n",
        "    _new = nei_out.copy()\n",
        "_new = _new.drop_duplicates(subset=['district','neighborhood','hashtags','source'])\n",
        "_new.to_csv(nei_long_path, index=False)\n",
        "# Update/append to neighborhoods wide table (idempotent)\n",
        "nei_wide_cols = ['district','neighborhood','green_area_km2','area_km2','green_share','green_share_label']\n",
        "nei_wide = nei[nei_wide_cols].copy()\n",
        "nei_wide_path = OUT_DIR/'berlin_neighborhoods_labels_wide.csv'\n",
        "if nei_wide_path.exists():\n",
        "    _nw = pd.read_csv(nei_wide_path)\n",
        "    _nw = _nw.merge(nei_wide, on=['district','neighborhood'], how='outer', suffixes=('', '_new'))\n",
        "    for c in ['green_area_km2','area_km2','green_share','green_share_label']:\n",
        "        if c+'_new' in _nw.columns:\n",
        "            _nw[c] = _nw[c].combine_first(_nw[c+'_new'])\n",
        "            _nw = _nw.drop(columns=[c+'_new'])\n",
        "else:\n",
        "    _nw = nei_wide\n",
        "_nw.to_csv(nei_wide_path, index=False)\n",
        "\n",
        "# District-level green share using parks + forest from regional statistics\n",
        "# Parks area by district (from neighborhoods aggregation)\n",
        "parks_area_km2_by_dist = nei.groupby('district', dropna=False)['green_area_km2'].sum().rename('parks_area_km2').reset_index()\n",
        "parks_area_ha_by_dist = parks_area_km2_by_dist.assign(green_area_parks_ha=lambda d: d['parks_area_km2']*100)[['district','green_area_parks_ha']]\n",
        "# Regional statistics: forest and total area (prefer 2024 if present; else latest)\n",
        "rs = pd.read_csv(REG_STATS_CSV)\n",
        "rs.columns = [c.strip().lower() for c in rs.columns]\n",
        "dcol = 'district' if 'district' in rs.columns else ('bezirk' if 'bezirk' in rs.columns else None)\n",
        "ycol = 'year' if 'year' in rs.columns else ('jahr' if 'jahr' in rs.columns else None)\n",
        "fcol = 'forest_area_ha' if 'forest_area_ha' in rs.columns else ('forest_ha' if 'forest_ha' in rs.columns else None)\n",
        "tcol = 'total_area_ha' if 'total_area_ha' in rs.columns else ('area_total_ha' if 'area_total_ha' in rs.columns else None)\n",
        "sfcol = 'share_forest' if 'share_forest' in rs.columns else None\n",
        "if dcol is None or (fcol is None and sfcol is None):\n",
        "    raise ValueError('regional_statistics.csv must provide district and either forest_area_ha or share_forest')\n",
        "if ycol and 2024 in set(pd.to_numeric(rs[ycol], errors='coerce').dropna().astype(int)):\n",
        "    rs = rs[pd.to_numeric(rs[ycol], errors='coerce').astype('Int64')==2024]\n",
        "elif ycol:\n",
        "    yy = pd.to_numeric(rs[ycol], errors='coerce').dropna().astype(int)\n",
        "    if not yy.empty:\n",
        "        latest = yy.max()\n",
        "        rs = rs[yy==latest]\n",
        "keep = [dcol] + ([fcol] if fcol else []) + ([tcol] if tcol else []) + ([sfcol] if sfcol else [])\n",
        "rs_slim = rs[keep].rename(columns={dcol:'district'})\n",
        "if fcol is None and sfcol and tcol:\n",
        "    rs_slim['forest_area_ha'] = pd.to_numeric(rs_slim[sfcol], errors='coerce') * pd.to_numeric(rs_slim[tcol], errors='coerce')\n",
        "else:\n",
        "    rs_slim['forest_area_ha'] = pd.to_numeric(rs_slim.get('forest_area_ha', np.nan), errors='coerce')\n",
        "if tcol:\n",
        "    rs_slim['total_area_ha'] = pd.to_numeric(rs_slim[tcol], errors='coerce')\n",
        "else:\n",
        "    dist_polys = GDF.dissolve(by=['district'], as_index=False)\n",
        "    total_km2 = dist_polys[['district','area_km2']].rename(columns={'area_km2':'total_area_km2'})\n",
        "    rs_slim = rs_slim.merge(total_km2, on='district', how='left')\n",
        "    rs_slim['total_area_ha'] = rs_slim['total_area_km2']*100\n",
        "# Merge parks area with regional stats\n",
        "dist2 = rs_slim.merge(parks_area_ha_by_dist, on='district', how='left').fillna({'green_area_parks_ha':0.0})\n",
        "dist2['green_area_total_ha'] = pd.to_numeric(dist2['green_area_parks_ha'], errors='coerce') + pd.to_numeric(dist2['forest_area_ha'], errors='coerce')\n",
        "dist2['green_share'] = (dist2['green_area_total_ha'] / pd.to_numeric(dist2['total_area_ha'], errors='coerce')).replace([np.inf,-np.inf], np.nan)\n",
        "# Label into three buckets\n",
        "medd = np.nanmedian(dist2['green_share']); l,u = medd-0.03, medd+0.03\n",
        "dist2['green_share_label'] = dist2['green_share'].apply(lambda v: ('low_green_share' if (pd.notna(v) and v<l) else ('high_green_share' if (pd.notna(v) and v>u) else 'average_green_share')))\n",
        "def to_tag(lbl):\n",
        "    return {'low_green_share':'#low_green_share','average_green_share':'#average_green_share','high_green_share':'#high_green_share'}.get(lbl, '#average_green_share')\n",
        "dist_out = pd.DataFrame({\n",
        "    'district': dist2['district'],\n",
        "    'hashtags': dist2['green_share_label'].map(to_tag),\n",
        "    'source': 'parks.ipynb:rule-based'\n",
        "})\n",
        "dist_out.to_csv(OUT_DIR/'district_labels_parks.csv', index=False)\n",
        "# Append to combined districts long (idempotent)\n",
        "dist_long_path = OUT_DIR/'berlin_districts_labels_long.csv'\n",
        "if dist_long_path.exists():\n",
        "    _old = pd.read_csv(dist_long_path)\n",
        "    _new = pd.concat([_old, dist_out], ignore_index=True)\n",
        "else:\n",
        "    _new = dist_out.copy()\n",
        "_new = _new.drop_duplicates(subset=['district','hashtags','source'])\n",
        "_new.to_csv(dist_long_path, index=False)\n",
        "# Update districts wide table\n",
        "dist_wide_cols = ['district','green_share','green_share_label','green_area_total_ha','forest_area_ha','green_area_parks_ha','total_area_ha']\n",
        "dist_wide = dist2[dist_wide_cols].copy()\n",
        "dist_wide_path = OUT_DIR/'berlin_districts_labels_wide.csv'\n",
        "if dist_wide_path.exists():\n",
        "    _wd = pd.read_csv(dist_wide_path)\n",
        "    _wd = _wd.merge(dist_wide, on=['district'], how='outer', suffixes=('', '_new'))\n",
        "    for c in ['green_share','green_share_label','green_area_total_ha','forest_area_ha','green_area_parks_ha','total_area_ha']:\n",
        "        if c+'_new' in _wd.columns:\n",
        "            _wd[c] = _wd[c].combine_first(_wd[c+'_new'])\n",
        "            _wd = _wd.drop(columns=[c+'_new'])\n",
        "else:\n",
        "    _wd = dist_wide\n",
        "_wd.to_csv(dist_wide_path, index=False)\n",
        "\n",
        "# Maps\n",
        "# Support both neighborhood labels (above/below/average) and district labels (high/average/low_green_share)\n",
        "CAT = {\n",
        "    'above average':'#1a9641',\n",
        "    'average':'#a6d96a',\n",
        "    'below average':'#fee08b',\n",
        "    'high_green_share':'#1a9641',\n",
        "    'average_green_share':'#a6d96a',\n",
        "    'low_green_share':'#fee08b',\n",
        "}\n",
        "def style_cat(f, col):\n",
        "    v = f['properties'].get(col); return {'fillColor': CAT.get(str(v).lower() if isinstance(v,str) else v, '#cccccc'), 'color':'#555','weight':0.5, 'fillOpacity':0.75}\n",
        "m_nei = folium.Map(location=[52.52,13.405], zoom_start=10, tiles='cartodbpositron')\n",
        "g_nei = GDF.merge(nei[['district','neighborhood','green_share_label']], on=['district','neighborhood'])\n",
        "folium.GeoJson(g_nei, style_function=lambda f, c='green_share_label': style_cat(f,c), tooltip=folium.GeoJsonTooltip(fields=['neighborhood','district','green_share_label'])).add_to(m_nei)\n",
        "m_nei.save(str(OUT_DIR/'parks_map_neighborhoods.html'))\n",
        "m_dist = folium.Map(location=[52.52,13.405], zoom_start=10, tiles='cartodbpositron')\n",
        "dist_polys = GDF.dissolve(by=['district'], as_index=False)\n",
        "g_dist = dist_polys.merge(dist2[['district','green_share_label']], on='district')\n",
        "folium.GeoJson(g_dist, style_function=lambda f, c='green_share_label': style_cat(f,c), tooltip=folium.GeoJsonTooltip(fields=['district','green_share_label'])).add_to(m_dist)\n",
        "m_dist.save(str(OUT_DIR/'parks_map_districts.html'))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
