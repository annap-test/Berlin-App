{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Venues & Vibrancy â€” Neighborhoods and Districts\n","Self-contained notebook to compute national cuisine diversity and vibrancy labels. Exports CSVs:\n","- neighborhoods: [district, neighborhood, hashtags, source]\n","- districts: [district, hashtags, source]"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "# Config (resolve project root so paths work from this notebook)\n",
    "ROOT = Path.cwd()\n",
    "if not (ROOT/'data').exists():\n",
    "    ROOT = ROOT.parent\n",
    "if not (ROOT/'data').exists():\n",
    "    raise FileNotFoundError(f\"Couldn't locate 'data' directory from {Path.cwd()}\")\n",
    "RAW_DIR = ROOT/'data'/'raw'\n",
    "NEI_PATH = ROOT/'data'/'neighborhoods.geojson'\n",
    "OUT_DIR = Path('outputs'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VENUES_CSV = RAW_DIR/'venues.csv'\n",
    "\n",
    "def ensure_wgs84(gdf):\n",
    "    if gdf.crs is None: return gdf.set_crs(4326)\n",
    "    return gdf.to_crs(4326) if gdf.crs.to_epsg()!=4326 else gdf\n",
    "def compute_area_km2(gdf):\n",
    "    gutm = ensure_wgs84(gdf).to_crs(25833)\n",
    "    gdf['area_km2'] = (gutm.geometry.area/1e6).values\n",
    "    gdf['area_eff_km2'] = gdf['area_km2'].clip(lower=0.20)\n",
    "    return gdf\n",
    "def percentile_score(s, lo=10, hi=90):\n",
    "    s = s.astype(float)\n",
    "    p_lo, p_hi = np.nanpercentile(s, lo), np.nanpercentile(s, hi)\n",
    "    rng = max(p_hi-p_lo, 1e-9)\n",
    "    return ((s - p_lo)/rng).clip(0,1)*100.0\n",
    "def hashtags(theme, label):\n",
    "    norm = str(label).strip().lower().replace(' ','-')\n",
    "    return ';'.join([f'#'+theme, f'#'+norm])\n",
    "\n",
    "# Simple national cuisine vocabulary\n",
    "NATIONALS = set('italian,french,spanish,portuguese,greek,turkish,german,polish,russian,ukrainian,balkan,hungarian,romanian,bulgarian,georgian,mexican,argentinian,peruvian,brazilian,colombian,venezuelan,caribbean,american,texmex,lebanese,israeli,palestinian,syrian,iraqi,iranian,afghan,moroccan,tunisian,algerian,ethiopian,eritrean,egyptian,southafrican,nigerian,indian,pakistani,bangladeshi,srilankan,nepali,chinese,japanese,korean,thai,vietnamese,laotian,cambodian,indonesian,malaysian,singaporean,filipino'.split(','))\n",
    "EXCLUDE = set('pizza,pasta,sushi,ramen,doner,dÃ¶ner,kebab,burger,bbq,grill,steak,noodles,dumpling,dumplings,sandwich,bakery,cafe,coffee,bubbletea,boba,falafel'.split(','))\n",
    "def tokenize(cuisines):\n",
    "    if pd.isna(cuisines): return []\n",
    "    toks = [''.join(ch for ch in str(p).lower().strip() if ch.isalnum()) for p in str(cuisines).split(';')]\n",
    "    return [t for t in toks if t and t not in EXCLUDE and t in NATIONALS]\n",
    "\n",
    "GDF = compute_area_km2(ensure_wgs84(gpd.read_file(NEI_PATH)))\n",
    "V = pd.read_csv(VENUES_CSV)\n",
    "V['tokens'] = V['cuisine'].apply(tokenize)\n",
    "nei = V.groupby(['district_id','neighborhood'], dropna=False).agg(n_venues=('cuisine','size'), n_types=('tokens', lambda s: len(set().union(*s)) if len(s) else 0)).reset_index()\n",
    "nei = GDF[['district_id','district','neighborhood','area_eff_km2']].merge(nei, on=['district_id','neighborhood'], how='left').fillna({'n_venues':0,'n_types':0})\n",
    "nei['venues_per_km2'] = (nei['n_venues']/nei['area_eff_km2']).replace([np.inf,-np.inf], np.nan)\n",
    "nei['V_score'] = percentile_score(nei['venues_per_km2']); nei['C_score'] = percentile_score(nei['n_types'])\n",
    "nei['VV_index'] = 0.65*nei['V_score'] + 0.35*nei['C_score']\n",
    "q1,q2 = np.nanpercentile(nei['VV_index'], 33.333), np.nanpercentile(nei['VV_index'], 66.666)\n",
    "nei['vibrancy_label'] = nei['VV_index'].apply(lambda v: 'vibrant' if (not np.isnan(v) and v>=q2) else ('sparse' if (not np.isnan(v) and v<=q1) else 'average'))\n",
    "nei_out = pd.DataFrame({\n",
    "    'district': nei['district'],\n",
    "    'neighborhood': nei['neighborhood'],\n",
    "    'hashtags': [hashtags('venues', v) for v in nei['vibrancy_label']],\n",
    "    'source': 'venues.ipynb:rule-based'\n",
    "})\n",
    "nei_out.to_csv(OUT_DIR/'neighborhood_labels_venues.csv', index=False)\n",
    "# Append to combined neighborhoods long (idempotent)\n",
    "nei_long_path = OUT_DIR/'berlin_neighborhoods_labels_long.csv'\n",
    "if nei_long_path.exists():\n",
    "    _old = pd.read_csv(nei_long_path)\n",
    "    _new = pd.concat([_old, nei_out], ignore_index=True)\n",
    "else:\n",
    "    _new = nei_out.copy()\n",
    "_new = _new.drop_duplicates(subset=['district','neighborhood','hashtags','source'])\n",
    "_new.to_csv(nei_long_path, index=False)\n",
    "# Update neighborhoods wide (idempotent)\n",
    "nei_wide_cols = ['district','neighborhood','area_eff_km2','n_venues','n_types','venues_per_km2','V_score','C_score','VV_index','vibrancy_label']\n",
    "nei_wide = nei[nei_wide_cols].copy()\n",
    "nei_wide_path = OUT_DIR/'berlin_neighborhoods_labels_wide.csv'\n",
    "if nei_wide_path.exists():\n",
    "    _nw = pd.read_csv(nei_wide_path)\n",
    "    _nw = _nw.merge(nei_wide, on=['district','neighborhood'], how='outer', suffixes=('', '_new'))\n",
    "    for c in ['area_eff_km2','n_venues','n_types','venues_per_km2','V_score','C_score','VV_index','vibrancy_label']:\n",
    "        if c+'_new' in _nw.columns:\n",
    "            _nw[c] = _nw[c].combine_first(_nw[c+'_new'])\n",
    "            _nw = _nw.drop(columns=[c+'_new'])\n",
    "else:\n",
    "    _nw = nei_wide\n",
    "_nw.to_csv(nei_wide_path, index=False)\n",
    "\n",
    "dist = V.groupby(['district_id'], dropna=False).agg(n_venues=('cuisine','size'), n_types=('tokens', lambda s: len(set().union(*s)) if len(s) else 0)).reset_index()\n",
    "dist = GDF[['district_id','district','area_eff_km2']].drop_duplicates('district_id').merge(dist, on='district_id', how='left').fillna({'n_venues':0,'n_types':0})\n",
    "dist['venues_per_km2'] = (dist['n_venues']/dist['area_eff_km2']).replace([np.inf,-np.inf], np.nan)\n",
    "dist['V_score'] = percentile_score(dist['venues_per_km2']); dist['C_score'] = percentile_score(dist['n_types'])\n",
    "dist['VV_index'] = 0.65*dist['V_score'] + 0.35*dist['C_score']\n",
    "q1d,q2d = np.nanpercentile(dist['VV_index'], 33.333), np.nanpercentile(dist['VV_index'], 66.666)\n",
    "dist['vibrancy_label'] = dist['VV_index'].apply(lambda v: 'vibrant' if (not np.isnan(v) and v>=q2d) else ('sparse' if (not np.isnan(v) and v<=q1d) else 'average'))\n",
    "dist_out = pd.DataFrame({\n",
    "    'district': dist['district'],\n",
    "    'hashtags': [hashtags('venues', v) for v in dist['vibrancy_label']],\n",
    "    'source': 'venues.ipynb:rule-based'\n",
    "})\n",
    "dist_out.to_csv(OUT_DIR/'district_labels_venues.csv', index=False)\n",
    "# Append to combined districts long (idempotent)\n",
    "dist_long_path = OUT_DIR/'berlin_districts_labels_long.csv'\n",
    "if dist_long_path.exists():\n",
    "    _old = pd.read_csv(dist_long_path)\n",
    "    _new = pd.concat([_old, dist_out], ignore_index=True)\n",
    "else:\n",
    "    _new = dist_out.copy()\n",
    "_new = _new.drop_duplicates(subset=['district','hashtags','source'])\n",
    "_new.to_csv(dist_long_path, index=False)\n",
    "# Update districts wide (idempotent)\n",
    "dist_wide_cols = ['district','area_eff_km2','n_venues','n_types','venues_per_km2','V_score','C_score','VV_index','vibrancy_label']\n",
    "dist_wide = dist[dist_wide_cols].copy()\n",
    "dist_wide_path = OUT_DIR/'berlin_districts_labels_wide.csv'\n",
    "if dist_wide_path.exists():\n",
    "    _wd = pd.read_csv(dist_wide_path)\n",
    "    _wd = _wd.merge(dist_wide, on=['district'], how='outer', suffixes=('', '_new'))\n",
    "    for c in ['area_eff_km2','n_venues','n_types','venues_per_km2','V_score','C_score','VV_index','vibrancy_label']:\n",
    "        if c+'_new' in _wd.columns:\n",
    "            _wd[c] = _wd[c].combine_first(_wd[c+'_new'])\n",
    "            _wd = _wd.drop(columns=[c+'_new'])\n",
    "else:\n",
    "    _wd = dist_wide\n",
    "_wd.to_csv(dist_wide_path, index=False)\n",
    "\n",
    "# Maps\n",
    "CAT = {'vibrant':'#1a9641','average':'#a6d96a','sparse':'#fee08b'}\n",
    "def style_cat(f, col):\n",
    "    v = f['properties'].get(col); return {'fillColor': CAT.get(str(v).lower() if isinstance(v,str) else v, '#cccccc'), 'color':'#555','weight':0.5, 'fillOpacity':0.75}\n",
    "m_nei = folium.Map(location=[52.52,13.405], zoom_start=10, tiles='cartodbpositron')\n",
    "g_nei = GDF.merge(nei[['district','neighborhood','vibrancy_label']], on=['district','neighborhood'])\n",
    "folium.GeoJson(g_nei, style_function=lambda f, c='vibrancy_label': style_cat(f,c), tooltip=folium.GeoJsonTooltip(fields=['neighborhood','district','vibrancy_label'])).add_to(m_nei)\n",
    "m_nei.save(str(OUT_DIR/'venues_map_neighborhoods.html'))\n",
    "m_dist = folium.Map(location=[52.52,13.405], zoom_start=10, tiles='cartodbpositron')\n",
    "dist_polys = GDF.dissolve(by=['district'], as_index=False)\n",
    "g_dist = dist_polys.merge(dist[['district','vibrancy_label']], on='district')\n",
    "folium.GeoJson(g_dist, style_function=lambda f, c='vibrancy_label': style_cat(f,c), tooltip=folium.GeoJsonTooltip(fields=['district','vibrancy_label'])).add_to(m_dist)\n",
    "m_dist.save(str(OUT_DIR/'venues_map_districts.html'))\n"
  ]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
