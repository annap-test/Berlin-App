{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "doc"},
      "source": [
        "# Documentation — Hashtags and Folium Map\n",
        "\n",
        "Computes mobility for neighborhoods and districts using U-Bahn stations and bus/tram stops and produces:\n",
        "- CSVs with `hashtags` like `#mobility;#well-connected` (districts omit neighborhood).\n",
        "- Folium maps at both levels.\n",
        "\n",
        "## Hashtag logic\n",
        "- Count features per neighborhood:\n",
        "  - `ubahn_stations` and `bus_tram_stops` from raw CSVs.\n",
        "  - Effective area `area_eff_km2` uses an area floor of 0.20 km² for stability.\n",
        "- Compute `connectivity_density` = (0.7 · ubahn + 0.3 · bus_tram) / area_eff_km2.\n",
        "- Convert to `mobility_score` via percentile scaling (p10→0, p90→100).\n",
        "- Classify by terciles of `mobility_score`: top → `well-connected`, mid → `moderate`, bottom → `remote`.\n",
        "- Export hashtags as `#mobility;#<label>` for neighborhoods and the aggregated district table.\n",
        "\n",
        "## Folium visualization\n",
        "- Build a `folium.Map` and add a `GeoJson` layer colored by `mobility_label`.\n",
        "- Tooltips show connection density and score where available; maps saved to `labels_with_visualization/outputs/mobility_map_*.html`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mobility Labels — Neighborhoods and Districts\n",
        "Self-contained notebook to compute rule-based mobility labels (U-Bahn + Bus/Tram) and export CSVs:\n",
        "- neighborhoods: columns [district, neighborhood, hashtags, source]\n",
        "- districts: columns [district, hashtags, source]\n",
        "Also saves two Folium maps (neighborhoods and districts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "DataSourceError",
          "evalue": "outputs\\neighborhoods.geojson: No such file or directory",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m+theme, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m+norm])\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m GDF = ensure_wgs84(\u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNEI_PATH\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     70\u001b[39m GDF = compute_area_km2(GDF)\n\u001b[32m     71\u001b[39m df_ubahn = pd.read_csv(UBAHN_CSV)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\projects\\Berlin-App\\.venv\\Lib\\site-packages\\geopandas\\io\\file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\projects\\Berlin-App\\.venv\\Lib\\site-packages\\geopandas\\io\\file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\projects\\Berlin-App\\.venv\\Lib\\site-packages\\pyogrio\\geopandas.py:275\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    274\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\projects\\Berlin-App\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1313\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:232\u001b[39m, in \u001b[36mpyogrio._io.ogr_open\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mDataSourceError\u001b[39m: outputs\\neighborhoods.geojson: No such file or directory"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import folium\n",
        "import branca\n",
        "\n",
        "# Config (resolve project root so paths work from this notebook)\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT/'data').exists():\n",
        "    ROOT = ROOT.parent\n",
        "if not (ROOT/'data').exists():\n",
        "    raise FileNotFoundError(f\"Couldn't locate 'data' directory from {Path.cwd()}\")\n",
        "RAW_DIR = ROOT/'data'/'raw'\n",
        "NEI_PATH = ROOT/'data'/'neighborhoods.geojson'\n",
        "OUT_DIR = Path('outputs'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "UBAHN_CSV = RAW_DIR/'ubahns.csv'\n",
        "BUS_TRAM_CSV = RAW_DIR/'bus_tram_stops.csv'\n",
        "\n",
        "# Helpers\n",
        "def ensure_wgs84(gdf):\n",
        "    if gdf.crs is None: return gdf.set_crs(4326)\n",
        "    return gdf.to_crs(4326) if gdf.crs.to_epsg()!=4326 else gdf\n",
        "\n",
        "def compute_area_km2(gdf):\n",
        "    gutm = ensure_wgs84(gdf).to_crs(25833)\n",
        "    gdf['area_km2'] = (gutm.geometry.area/1e6).values\n",
        "    gdf['area_eff_km2'] = gdf['area_km2'].clip(lower=0.20)\n",
        "    return gdf\n",
        "\n",
        "def to_points(df):\n",
        "    lat = next(c for c in df.columns if c.lower() in {'lat','latitude','y'})\n",
        "    lon = next(c for c in df.columns if c.lower() in {'lon','lng','long','longitude','x'})\n",
        "    g = gpd.GeoDataFrame(df.copy(), geometry=[Point(xy) for xy in zip(df[lon], df[lat])], crs=4326)\n",
        "    return ensure_wgs84(g)\n",
        "\n",
        "def percentile_score(s, lo=10, hi=90):\n",
        "    s = s.astype(float)\n",
        "    p_lo, p_hi = np.nanpercentile(s, lo), np.nanpercentile(s, hi)\n",
        "    rng = max(p_hi-p_lo, 1e-9)\n",
        "    return ((s - p_lo)/rng).clip(0,1)*100.0\n",
        "\n",
        "def tercile_label(v, q1, q2, hi_label, mid_label, lo_label):\n",
        "    if np.isnan(v): return mid_label\n",
        "    if v >= q2: return hi_label\n",
        "    if v <= q1: return lo_label\n",
        "    return mid_label\n",
        "\n",
        "def compute_labels_neighborhoods(gdf_nei, df_ubahn, df_bus):\n",
        "    gdf = compute_area_km2(ensure_wgs84(gdf_nei.copy()))\n",
        "    g_u = to_points(df_ubahn); g_b = to_points(df_bus)\n",
        "    # spatial join by polygon index\n",
        "    base = gdf.reset_index().rename(columns={'index':'poly_index'})\n",
        "    polys = base[['poly_index','geometry']]\n",
        "    pts_u = gpd.sjoin(g_u, polys, how='inner', predicate='within')\n",
        "    pts_b = gpd.sjoin(g_b, polys, how='inner', predicate='within')\n",
        "    cu = pts_u.groupby('poly_index').size().rename('ubahn_stations').reset_index()\n",
        "    cb = pts_b.groupby('poly_index').size().rename('bus_tram_stops').reset_index()\n",
        "    out = base.merge(cu, on='poly_index', how='left').merge(cb, on='poly_index', how='left')\n",
        "    out[['ubahn_stations','bus_tram_stops']] = out[['ubahn_stations','bus_tram_stops']].fillna(0).astype(int)\n",
        "    out['total_stops'] = out['ubahn_stations'] + out['bus_tram_stops']\n",
        "    out['connectivity_density'] = (0.7*out['ubahn_stations'] + 0.3*out['bus_tram_stops'])/out['area_eff_km2']\n",
        "    out['mobility_score'] = percentile_score(out['connectivity_density'])\n",
        "    q1, q2 = np.nanpercentile(out['mobility_score'], 33.333), np.nanpercentile(out['mobility_score'], 66.666)\n",
        "    out['mobility_label'] = [tercile_label(v,q1,q2,'well-connected','moderate','remote') for v in out['mobility_score']]\n",
        "    return out\n",
        "\n",
        "def hashtags_from_label(theme, label):\n",
        "    norm = str(label).strip().lower().replace(' ','-')\n",
        "    return ';'.join([f'#'+theme, f'#'+norm])\n",
        "\n",
        "# Load data\n",
        "GDF = ensure_wgs84(gpd.read_file(NEI_PATH))\n",
        "GDF = compute_area_km2(GDF)\n",
        "df_ubahn = pd.read_csv(UBAHN_CSV)\n",
        "df_bus = pd.read_csv(BUS_TRAM_CSV)\n",
        "\n",
        "nei_labels = compute_labels_neighborhoods(GDF, df_ubahn, df_bus)\n",
        "# Neighborhood output table\n",
        "nei_out = pd.DataFrame({\n",
        "    'district': nei_labels['district'] if 'district' in nei_labels.columns else nei_labels['district_id'].astype(str),\n",
        "    'neighborhood': nei_labels['neighborhood'],\n",
        "    'hashtags': [hashtags_from_label('mobility', v) for v in nei_labels['mobility_label']],\n",
        "    'source': 'mobility.ipynb:rule-based'\n",
        "})\n",
        "nei_out.to_csv(OUT_DIR/'neighborhood_labels_mobility.csv', index=False)\n",
        "# Append to combined neighborhoods long (idempotent)\n",
        "nei_long_path = OUT_DIR/'berlin_neighborhoods_labels_long.csv'\n",
        "if nei_long_path.exists():\n",
        "    _old = pd.read_csv(nei_long_path)\n",
        "    _new = pd.concat([_old, nei_out], ignore_index=True)\n",
        "else:\n",
        "    _new = nei_out.copy()\n",
        "_new = _new.drop_duplicates(subset=['district','neighborhood','hashtags','source'])\n",
        "_new.to_csv(nei_long_path, index=False)\n",
        "# Update neighborhoods wide (idempotent)\n",
        "nei_wide_cols = ['district','neighborhood','ubahn_stations','bus_tram_stops','area_eff_km2','connectivity_density','mobility_score','mobility_label']\n",
        "nei_wide = nei_labels[nei_wide_cols].copy()\n",
        "nei_wide_path = OUT_DIR/'berlin_neighborhoods_labels_wide.csv'\n",
        "if nei_wide_path.exists():\n",
        "    _nw = pd.read_csv(nei_wide_path)\n",
        "    _nw = _nw.merge(nei_wide, on=['district','neighborhood'], how='outer', suffixes=('', '_new'))\n",
        "    for c in ['ubahn_stations','bus_tram_stops','area_eff_km2','connectivity_density','mobility_score','mobility_label']:\n",
        "        if c+'_new' in _nw.columns:\n",
        "            _nw[c] = _nw[c].combine_first(_nw[c+'_new'])\n",
        "            _nw = _nw.drop(columns=[c+'_new'])\n",
        "else:\n",
        "    _nw = nei_wide\n",
        "_nw.to_csv(nei_wide_path, index=False)\n",
        "\n",
        "# District aggregation\n",
        "grp = nei_labels.groupby(['district','district_id'] if 'district' in nei_labels.columns else ['district_id'], dropna=False)[['ubahn_stations','bus_tram_stops','area_eff_km2']].sum().reset_index()\n",
        "grp['connectivity_density'] = (0.7*grp['ubahn_stations'] + 0.3*grp['bus_tram_stops'])/grp['area_eff_km2']\n",
        "grp['mobility_score'] = percentile_score(grp['connectivity_density'])\n",
        "q1d, q2d = np.nanpercentile(grp['mobility_score'], 33.333), np.nanpercentile(grp['mobility_score'], 66.666)\n",
        "grp['mobility_label'] = [tercile_label(v,q1d,q2d,'well-connected','moderate','remote') for v in grp['mobility_score']]\n",
        "dist_out = pd.DataFrame({\n",
        "    'district': grp['district'] if 'district' in grp.columns else grp['district_id'].astype(str),\n",
        "    'hashtags': [hashtags_from_label('mobility', v) for v in grp['mobility_label']],\n",
        "    'source': 'mobility.ipynb:rule-based'\n",
        "})\n",
        "dist_out.to_csv(OUT_DIR/'district_labels_mobility.csv', index=False)\n",
        "# Append to combined districts long (idempotent)\n",
        "dist_long_path = OUT_DIR/'berlin_districts_labels_long.csv'\n",
        "if dist_long_path.exists():\n",
        "    _old = pd.read_csv(dist_long_path)\n",
        "    _new = pd.concat([_old, dist_out], ignore_index=True)\n",
        "else:\n",
        "    _new = dist_out.copy()\n",
        "_new = _new.drop_duplicates(subset=['district','hashtags','source'])\n",
        "_new.to_csv(dist_long_path, index=False)\n",
        "# Update districts wide (idempotent)\n",
        "dist_wide_cols = ['district','ubahn_stations','bus_tram_stops','area_eff_km2','connectivity_density','mobility_score','mobility_label']\n",
        "grp2 = grp.copy()\n",
        "if 'district' not in grp2.columns and 'district_id' in grp2.columns:\n",
        "    # If no district name column, derive a string id for key consistency\n",
        "    grp2['district'] = grp2['district_id'].astype(str)\n",
        "dist_wide = grp2[['district'] + dist_wide_cols[1:]].copy()\n",
        "dist_wide_path = OUT_DIR/'berlin_districts_labels_wide.csv'\n",
        "if dist_wide_path.exists():\n",
        "    _wd = pd.read_csv(dist_wide_path)\n",
        "    _wd = _wd.merge(dist_wide, on=['district'], how='outer', suffixes=('', '_new'))\n",
        "    for c in ['ubahn_stations','bus_tram_stops','area_eff_km2','connectivity_density','mobility_score','mobility_label']:\n",
        "        if c+'_new' in _wd.columns:\n",
        "            _wd[c] = _wd[c].combine_first(_wd[c+'_new'])\n",
        "            _wd = _wd.drop(columns=[c+'_new'])\n",
        "else:\n",
        "    _wd = dist_wide\n",
        "_wd.to_csv(dist_wide_path, index=False)\n",
        "\n",
        "# Folium maps\n",
        "CAT_PALETTE = {'well-connected':'#1a9641','moderate':'#a6d96a','remote':'#fee08b'}\n",
        "def style_cat(feature, column):\n",
        "    v = feature['properties'].get(column)\n",
        "    color = CAT_PALETTE.get(str(v).lower(), '#cccccc')\n",
        "    return {'fillColor': color, 'color': '#555555', 'weight': 0.5, 'fillOpacity': 0.75}\n",
        "\n",
        "# Neighborhood map\n",
        "nei_map = folium.Map(location=[52.52,13.405], zoom_start=10, tiles='cartodbpositron')\n",
        "g_nei = GDF.merge(nei_labels[['neighborhood','district','mobility_label']], on=['neighborhood','district']) if 'district' in GDF.columns else GDF.merge(nei_labels[['neighborhood','mobility_label']], on='neighborhood')\n",
        "folium.GeoJson(g_nei, style_function=lambda f, col='mobility_label': style_cat(f,col), tooltip=folium.GeoJsonTooltip(fields=[c for c in ['neighborhood','district','mobility_label'] if c in g_nei.columns])).add_to(nei_map)\n",
        "nei_map.save(str(OUT_DIR/'mobility_map_neighborhoods.html'))\n",
        "\n",
        "# District map\n",
        "dist_polys = GDF.dissolve(by=['district','district_id'] if 'district' in GDF.columns else ['district_id'], as_index=False)\n",
        "g_dist = dist_polys.merge(grp[['district','district_id','mobility_label']] if 'district' in dist_polys.columns else grp[['district_id','mobility_label']], on=['district','district_id'] if 'district' in dist_polys.columns else ['district_id'])\n",
        "dist_map = folium.Map(location=[52.52,13.405], zoom_start=10, tiles='cartodbpositron')\n",
        "folium.GeoJson(g_dist, style_function=lambda f, col='mobility_label': style_cat(f,col), tooltip=folium.GeoJsonTooltip(fields=[c for c in ['district','mobility_label'] if c in g_dist.columns])).add_to(dist_map)\n",
        "dist_map.save(str(OUT_DIR/'mobility_map_districts.html'))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
